{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "935cd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'oz_questions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81949b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth();\n",
    "gauth.LoadClientConfigFile(\"client_secrets.json\");\n",
    "gauth.LocalWebserverAuth();\n",
    "\n",
    "drive = GoogleDrive(gauth);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1561747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    os.remove(filepath)\n",
    "    print(f\"Replacing existing file: {filepath}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "file = drive.CreateFile({'id': '1aa61xgSOBXu6qH1chEiFqUxgFmBNyvr4Q5bSCFUOkt8'})\n",
    "file.GetContentFile(filepath, mimetype='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a8546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oz_q_df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae6ae5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Book #</th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Best Answer</th>\n",
       "      <th>length</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What color are Dorothy's shoes?</td>\n",
       "      <td>1</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>“She was so old,” explained the Witch of the N...</td>\n",
       "      <td>285.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How old is the Scarecrow when Dorothy finds him?</td>\n",
       "      <td>1</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>“My life has been so short that I really know ...</td>\n",
       "      <td>412.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which are the first antagonistic creatures the...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>In the morning they traveled on until they cam...</td>\n",
       "      <td>1085.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When is the first time we read \"There's no pla...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>“That is because you have no brains” answered ...</td>\n",
       "      <td>236.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the wizard's secret in the Wonderful W...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>“No, you are all wrong,” said the little man m...</td>\n",
       "      <td>546.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  Book #  \\\n",
       "0                    What color are Dorothy's shoes?       1   \n",
       "1   How old is the Scarecrow when Dorothy finds him?       1   \n",
       "2  Which are the first antagonistic creatures the...       1   \n",
       "3  When is the first time we read \"There's no pla...       1   \n",
       "4  What is the wizard's secret in the Wonderful W...       1   \n",
       "\n",
       "                   Book Title  \\\n",
       "0  The Wonderful Wizard of Oz   \n",
       "1  The Wonderful Wizard of Oz   \n",
       "2  The Wonderful Wizard of Oz   \n",
       "3  The Wonderful Wizard of Oz   \n",
       "4  The Wonderful Wizard of Oz   \n",
       "\n",
       "                                         Best Answer  length Note  \n",
       "0  “She was so old,” explained the Witch of the N...   285.0  NaN  \n",
       "1  “My life has been so short that I really know ...   412.0  NaN  \n",
       "2  In the morning they traveled on until they cam...  1085.0  NaN  \n",
       "3  “That is because you have no brains” answered ...   236.0  NaN  \n",
       "4  “No, you are all wrong,” said the little man m...   546.0  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oz_q_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7bf8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 16 questions from books (2 per book)\n",
      "Selected 4 additional random questions\n",
      "Total: 20 questions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def select_questions(oz_q_df, random_seed=42,\n",
    "                     questions_per_book=2, additional_random_questions=4):\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get unique books\n",
    "    unique_books = oz_q_df['Book #'].unique()\n",
    "\n",
    "    # Select 2 questions from each book\n",
    "    selected_per_book = []\n",
    "    for book_num in unique_books:\n",
    "        book_questions = oz_q_df[oz_q_df['Book #'] == book_num]\n",
    "        sample = book_questions.sample(n=min(questions_per_book, len(book_questions)), random_state=random_seed)\n",
    "        selected_per_book.append(sample)\n",
    "\n",
    "    selected_from_books = pd.concat(selected_per_book)\n",
    "\n",
    "    # Select 4 additional random questions from remaining questions\n",
    "    remaining_questions = oz_q_df[~oz_q_df.index.isin(selected_from_books.index)]\n",
    "    additional_questions = remaining_questions.sample(n=min(additional_random_questions, len(remaining_questions)), random_state=random_seed)\n",
    "\n",
    "    # Combine all selected questions\n",
    "    final_selection = pd.concat([selected_from_books, additional_questions])\n",
    "\n",
    "    print(f\"Selected {len(selected_from_books)} questions from books ({questions_per_book} per book)\")\n",
    "    print(f\"Selected {len(additional_questions)} additional random questions\")\n",
    "    print(f\"Total: {len(final_selection)} questions\\n\")\n",
    "\n",
    "    return final_selection\n",
    "\n",
    "selected_questions = select_questions(oz_q_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89032e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API PARAMETERS\n",
    "BASE_URL = \"http://localhost:8080\"\n",
    "TEST_URL = \"https://www.gutenberg.org/cache/epub/55/pg55-images.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa5309",
   "metadata": {},
   "source": [
    "#### IMPORTANT\n",
    "Start up your backend locally for testing\n",
    "> run docker compose up --build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "072eb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "# List of book URLs\n",
    "BOOK_URLS = [\n",
    "    \"https://www.gutenberg.org/cache/epub/55/pg55-images.html\",       # The Wonderful Wizard of Oz\n",
    "    \"https://www.gutenberg.org/cache/epub/54/pg54-images.html\",       # Marvelous Land of Oz\n",
    "    \"https://www.gutenberg.org/cache/epub/33361/pg33361-images.html\", # Ozma of Oz\n",
    "    \"https://www.gutenberg.org/cache/epub/22566/pg22566-images.html\", # Dorothy and the Wizard in Oz\n",
    "    \"https://www.gutenberg.org/cache/epub/26624/pg26624-images.html\", # The Road to Oz\n",
    "    \"https://www.gutenberg.org/cache/epub/41667/pg41667-images.html\", # The Emerald City of Oz\n",
    "    \"https://www.gutenberg.org/cache/epub/32094/pg32094-images.html\", # The Patchwork Girl of Oz\n",
    "    \"https://www.gutenberg.org/cache/epub/75720/pg75720-images.html\", # Jack Pumpkinhead of Oz\n",
    "]\n",
    "\n",
    "async def process_book(session, url, target_chunk_size=800, sentence_overlap=2,\n",
    "                       small_paragraph_length=200, small_paragraph_overlap=2):\n",
    "    \"\"\"Process a single book and return its filename\"\"\"\n",
    "    book_data_payload = {\n",
    "        \"url\": url,\n",
    "        \"target_chunk_size\": target_chunk_size,\n",
    "        \"sentence_overlap\": sentence_overlap,\n",
    "        \"small_paragraph_length\": small_paragraph_length,\n",
    "        \"small_paragraph_overlap\": small_paragraph_overlap\n",
    "    }\n",
    "\n",
    "    async with session.post(f\"{BASE_URL}/v1/book-data\", json=book_data_payload) as response:\n",
    "        result = await response.json()\n",
    "        if result.get(\"status\") == \"error\":\n",
    "            raise Exception(f\"Error uploading {url}: {result['message']}\")\n",
    "        return result.get(\"filename\")\n",
    "\n",
    "\n",
    "async def process_all_books(book_urls, **chunking_params):\n",
    "    \"\"\"\n",
    "    Process all books in parallel and wait for all to complete\n",
    "\n",
    "    This mimics Promise.all in JavaScript (as done in the frontend)\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create tasks for all uploads\n",
    "        tasks = [\n",
    "            process_book(session, url, **chunking_params)\n",
    "            for url in book_urls\n",
    "        ]\n",
    "\n",
    "        filenames = []\n",
    "        # Process tasks as they complete\n",
    "        for i, coroutine in enumerate(asyncio.as_completed(tasks), 1):\n",
    "            filename = await coroutine\n",
    "            print(f\"[{i}/{len(tasks)}] Completed chunking and embedding: {filename}\")\n",
    "            filenames.append(filename)\n",
    "\n",
    "        print(f\"Successfully processed {len(filenames)} books\")\n",
    "        return filenames\n",
    "\n",
    "\n",
    "async def run_test_async(\n",
    "    session,\n",
    "    test_query=None,\n",
    "    filenames=None,\n",
    "    book_urls=None,\n",
    "    target_chunk_size=None,\n",
    "    sentence_overlap=None,\n",
    "    small_paragraph_length=None,\n",
    "    small_paragraph_overlap=None,\n",
    "    skip_book_upload=True\n",
    "):\n",
    "\n",
    "    if not test_query:\n",
    "        raise ValueError(\"test_query must be provided\")\n",
    "\n",
    "    if not skip_book_upload:\n",
    "        if any(param is None for param in [\n",
    "            target_chunk_size, sentence_overlap,\n",
    "            small_paragraph_length, small_paragraph_overlap,\n",
    "            book_urls\n",
    "        ]):\n",
    "            raise ValueError(\"Chunking parameters must be provided when skip_book_upload is False\")\n",
    "\n",
    "        filenames = await process_all_books(\n",
    "            book_urls=book_urls,\n",
    "            target_chunk_size=target_chunk_size,\n",
    "            sentence_overlap=sentence_overlap,\n",
    "            small_paragraph_length=small_paragraph_length,\n",
    "            small_paragraph_overlap=small_paragraph_overlap\n",
    "        )\n",
    "    elif not filenames:\n",
    "        raise ValueError(\"filenames must be provided if skip_book_upload is True\")\n",
    "\n",
    "    query_id = str(uuid.uuid4())\n",
    "    model_payload = {\"user_query\": test_query}\n",
    "\n",
    "    async with session.post(f\"{BASE_URL}/v1/model-response\", json=model_payload) as response:\n",
    "        result = await response.json()\n",
    "        if result.get(\"status\") == \"error\":\n",
    "            raise Exception(f\"Error in model response: {result['message']}\")\n",
    "\n",
    "    search_payload = {\n",
    "        \"query\": result[\"search_query\"],\n",
    "        \"filenames\": filenames,\n",
    "        \"top_k\": 3,\n",
    "        \"query_id\": query_id,\n",
    "        \"enhanced_query\": True\n",
    "    }\n",
    "\n",
    "    async with session.post(f\"{BASE_URL}/v1/search-response\", json=search_payload) as response:\n",
    "        search_results = await response.json()\n",
    "        if search_results.get(\"status\") == \"error\":\n",
    "            raise Exception(f\"Error in search response: {search_results['message']}\")\n",
    "        return search_results, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter combinations to test\n",
    "PARAM_COMBOS = [\n",
    "    # (target_chunk_size, sentence_overlap, small_paragraph_length, small_paragraph_overlap)\n",
    "    (800, 2, 200, 2),   # Default\n",
    "    (1000, 2, 200, 2),  # Larger chunks\n",
    "    (1000, 2, 200, 3),  # Larger chunks with more paragraph overlap\n",
    "    (1000, 2, 150, 3),  # Larger chunks with smaller paragraph threshold\n",
    "    (600, 2, 200, 2),   # Smaller chunks\n",
    "    (600, 2, 200, 3),   # Smaller chunks with more paragraph overlap\n",
    "    (600, 2, 150, 3),   # Smaller chunks with smaller\n",
    "    (600, 2, 250, 3),   # Smaller chunks with larger paragraph threshold\n",
    "    (700, 2, 200, 2),   # Medium chunks\n",
    "    (700, 2, 200, 3),   # Medium chunks with more paragraph overlap\n",
    "    (700, 2, 150, 3),   # Medium chunks with smaller paragraph threshold\n",
    "    (800, 3, 200, 2),   # More sentence overlap\n",
    "    (800, 1, 200, 2),   # Less sentence overlap\n",
    "    (800, 2, 150, 2),   # Smaller paragraph threshold\n",
    "    (800, 2, 250, 2),   # Larger paragraph threshold\n",
    "    (800, 2, 200, 3),   # More paragraph overlap\n",
    "    (800, 2, 200, 1),   # Less paragraph overlap\n",
    "]\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text.lower().replace('“', '\"').replace('”', '\"').strip())\n",
    "\n",
    "def get_first_sentence_partial_match(text):\n",
    "    return text.replace('“', '\"').replace('”', '\"').lower().split(\"\\n\")[0]\n",
    "\n",
    "def find_chunk_location_from_text(filepath, expected_text, partial_match_ratio=3):\n",
    "    temp_df = pd.read_pickle(filepath)\n",
    "\n",
    "    for chunk_idx, row in temp_df.iterrows():\n",
    "        normalized_expected_text = normalize_text(expected_text)\n",
    "        normalized_df_text = normalize_text(row['text'])\n",
    "\n",
    "        full_text_location_match = normalized_expected_text in normalized_df_text\n",
    "        partial_text_location_match = (\n",
    "            normalized_expected_text[:len(normalized_expected_text)//partial_match_ratio] in normalized_df_text\n",
    "            or normalized_expected_text in normalized_df_text[-len(normalized_expected_text)//partial_match_ratio:]\n",
    "        )\n",
    "        first_sentence_partial_match = get_first_sentence_partial_match(expected_text) in normalized_df_text\n",
    "\n",
    "        if partial_text_location_match and not full_text_location_match:\n",
    "            print(\"NOTE: taking first chunk location where ~partial~ text matched.\")\n",
    "\n",
    "        if first_sentence_partial_match and not (full_text_location_match or partial_text_location_match):\n",
    "            print(\"NOTE: taking first chunk location where ~first sentence partial~ text matched.\")\n",
    "\n",
    "        if full_text_location_match or partial_text_location_match or first_sentence_partial_match:\n",
    "            chapter_number = int(row['chapter_index']) # potential to change this name\n",
    "            chapter_title = str(row['title'])\n",
    "            chunk_in_chapter_match = re.search(r\"(.*)\\((\\d+)\\)\", chapter_title)\n",
    "            if chunk_in_chapter_match and chunk_in_chapter_match.group(2).isdigit():\n",
    "                chunk_in_chapter_index = int(chunk_in_chapter_match.group(2))\n",
    "                chapter_title = chunk_in_chapter_match.group(1).strip()\n",
    "            else:\n",
    "                chunk_in_chapter_index = None\n",
    "            return chunk_idx, chapter_number, chapter_title, chunk_in_chapter_index\n",
    "\n",
    "    print(\"WARNING: no chunk was matched to the given text in 'BEST ANSWER'!\")\n",
    "    return (None, None, None, None)\n",
    "\n",
    "async def run_all_tests(\n",
    "    selected_questions,\n",
    "    param_combos:list=PARAM_COMBOS,\n",
    "    book_urls:list=BOOK_URLS,\n",
    "    skip_book_processing:bool=False,\n",
    "):\n",
    "    run_id = str(uuid.uuid4())[:8]\n",
    "    import datetime\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for test_num, params in enumerate(param_combos, 1):\n",
    "            target_chunk_size, sentence_overlap, small_paragraph_length, small_paragraph_overlap = params\n",
    "\n",
    "            timestamp = datetime.datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "            print(f\"[{timestamp}] [{run_id}] TEST {test_num}: chunk_size={target_chunk_size}\")\n",
    "\n",
    "            if not skip_book_processing:\n",
    "                start_time = datetime.datetime.now()\n",
    "                filenames = await process_all_books(\n",
    "                    book_urls=book_urls,\n",
    "                    target_chunk_size=target_chunk_size,\n",
    "                    sentence_overlap=sentence_overlap,\n",
    "                    small_paragraph_length=small_paragraph_length,\n",
    "                    small_paragraph_overlap=small_paragraph_overlap\n",
    "                )\n",
    "                end_time = datetime.datetime.now()\n",
    "                elapsed = (end_time - start_time).total_seconds()\n",
    "                print(f\"[{timestamp}] [{run_id}] Completed book processing for TEST {test_num}, \"\n",
    "                  f\"it took {elapsed:.2f} seconds\")\n",
    "            else:\n",
    "                print(\n",
    "                    \"Skipping book processing. Assumption is dfs are already processed correctly and available in the directory.\"\n",
    "                )\n",
    "                # fallback to hardcoded filenames\n",
    "                filenames = [\n",
    "                    \"the_wonderful_wizard_of_oz\",\n",
    "                    \"the_marvelous_land_of_oz\",\n",
    "                    \"ozma_of_oz\",\n",
    "                    \"dorothy_and_the_wizard_in_oz\",\n",
    "                    \"the_road_to_oz\",\n",
    "                    \"the_emerald_city_of_oz\",\n",
    "                    \"the_patchwork_girl_of_oz\",\n",
    "                    \"jack_pumpkinhead_of_oz\",\n",
    "                ]\n",
    "\n",
    "            print(f\"[{timestamp}] [{run_id}] Starting tests per query for TEST {test_num}\")\n",
    "\n",
    "            for question_number, (_, question_row) in enumerate(selected_questions.iterrows(), 1):\n",
    "\n",
    "                question = question_row['Question']\n",
    "                print(f\"[{timestamp}] [{run_id}]   Question {question_number}: {question}\")\n",
    "\n",
    "                search_results, _ = await run_test_async(\n",
    "                    session,\n",
    "                    test_query=question,\n",
    "                    filenames=filenames\n",
    "                )\n",
    "\n",
    "                # Process results...\n",
    "                chunk_lengths = {}\n",
    "                avg_chunk_length = {}\n",
    "                for filename in filenames:\n",
    "                    temp_df = pd.read_pickle(f\"../temp/{filename}.pkl\")\n",
    "                    chunk_lengths.update({filename: temp_df.chunk_length.tolist()})\n",
    "                    avg_chunk_length.update({filename: temp_df.chunk_length.mean()})\n",
    "\n",
    "                # Find the expected chunk index\n",
    "                expected_filename = question_row['Book Title'].lower().replace(' ', '_').strip()\n",
    "                expected_book_filepath = f\"../temp/{expected_filename}.pkl\"\n",
    "\n",
    "                (\n",
    "                    expected_chunk_index,\n",
    "                    expected_chapter_number,\n",
    "                    expected_chapter_title,\n",
    "                    expected_chunk_in_chapter_index\n",
    "                ) = find_chunk_location_from_text(\n",
    "                        expected_book_filepath, question_row['Best Answer']\n",
    "                    ) if os.path.exists(expected_book_filepath) else (None, None, None, None)\n",
    "\n",
    "                for result_num, result in enumerate(search_results[\"search_results\"], 1):\n",
    "                    matched_text = result['data']['matched_texts']\n",
    "                    matched_hits = [m.get('text', '') for m in matched_text if m.get('is_match') is True]\n",
    "                    match_text = \" \".join(matched_hits) if matched_hits else \"\"\n",
    "\n",
    "                    book_match = result['data']['book_title'].lower().strip() == question_row['Book Title'].lower().strip()\n",
    "                    text_match = question_row['Best Answer'] in match_text\n",
    "\n",
    "                    # chunk distances\n",
    "                    chunk_distance_from_expected = abs(\n",
    "                        result['data']['chunk_index'] - expected_chunk_index\n",
    "                        ) if expected_chunk_index is not None and book_match else \"NaN\"\n",
    "                    character_distance_from_expected = (round(chunk_distance_from_expected * avg_chunk_length[expected_filename])) if os.path.exists(expected_book_filepath) and chunk_distance_from_expected != \"NaN\" else \"NaN\"\n",
    "\n",
    "                    # chapter distances\n",
    "                    chapter_match = (result['data']['chapter_number'] == expected_chapter_number) if book_match else \"NaN\"\n",
    "                    chapter_distance_from_expected = abs(\n",
    "                        result['data']['chapter_number'] - expected_chapter_number\n",
    "                        ) if expected_chapter_number is not None and book_match else \"NaN\"\n",
    "                    chunk_in_chap_distance_from_expected = abs(\n",
    "                        result['data']['chunk_in_chapter_index'] - expected_chunk_in_chapter_index\n",
    "                    ) if expected_chunk_in_chapter_index is not None and chapter_match is True else \"NaN\"\n",
    "\n",
    "                    result_row = {\n",
    "                        'test_number': test_num,\n",
    "                        'question_number': question_number,\n",
    "                        'result_rank': result_num,\n",
    "                        'original_query': question,\n",
    "                        'enhanced_query': result['data']['query'],\n",
    "                        'target_chunk_size': target_chunk_size,\n",
    "                        'sentence_overlap': sentence_overlap,\n",
    "                        'small_paragraph_length': small_paragraph_length,\n",
    "                        'small_paragraph_overlap': small_paragraph_overlap,\n",
    "                        'matched_chapter_title': result['data']['chapter_title'],\n",
    "                        'expected_chapter_title': expected_chapter_title,\n",
    "                        'matched_chapter_number': result['data']['chapter_number'],\n",
    "                        'expected_chapter_number': expected_chapter_number,\n",
    "                        'correct_chapter_found': chapter_match,\n",
    "                        'chapter_distance_from_expected': chapter_distance_from_expected,\n",
    "                        'matched_chunk_in_chapter_index': result['data']['chunk_in_chapter_index'],\n",
    "                        'exptected_chunk_in_chapter_index': expected_chunk_in_chapter_index,\n",
    "                        'chunk_in_chap_distance_from_expected': chunk_in_chap_distance_from_expected,\n",
    "                        'score': result['data']['score'],\n",
    "                        'matched_text': match_text,\n",
    "                        'expected_text': question_row['Best Answer'],\n",
    "                        'correct_text_found': text_match,\n",
    "                        'matched_chunk_index': result['data']['chunk_index'],\n",
    "                        'expected_chunk_index': expected_chunk_index if expected_chunk_index else \"NaN\",\n",
    "                        'chunk_distance_from_expected': chunk_distance_from_expected,\n",
    "                        'char_distance_from_expected': character_distance_from_expected,\n",
    "                        'matched_book_title': result['data']['book_title'],\n",
    "                        'expected_book_title': question_row['Book Title'],\n",
    "                        'correct_book_found': book_match,\n",
    "                        'avg_chunk_length': avg_chunk_length,\n",
    "                        'all_chunks': chunk_lengths,\n",
    "                    }\n",
    "                    all_results.append(result_row)\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{timestamp}] [{run_id}] COMPLETED {len(param_combos)} TESTS\")\n",
    "    print(f\"[{timestamp}] [{run_id}] Total results collected: {len(results_df)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "947791a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:39:48.682] [425d6589] TEST 1: chunk_size=800\n",
      "Skipping book processing. Assumption is dfs are already processed correctly and available in the directory.\n",
      "[00:39:48.682] [425d6589] Starting tests per query for TEST 1\n",
      "[00:39:48.682] [425d6589]   Question 1: How is Glinda's appearance described when Dorothy meets her?\n",
      "[00:39:48.682] [425d6589]   Question 2: How old is the Scarecrow when Dorothy finds him?\n",
      "[00:39:48.682] [425d6589]   Question 3: How does one use the Powder of Life?\n",
      "NOTE: taking first chunk location where ~partial~ text matched.\n",
      "[00:39:48.682] [425d6589]   Question 4: When is Wogglebug introduced?\n",
      "[00:39:48.682] [425d6589]   Question 5: What happens to the Scarecrow and the Sawhorse that they get hurt?\n",
      "[00:39:48.682] [425d6589]   Question 6: When do Dorothy and Ozma meet?\n",
      "[00:39:48.682] [425d6589]   Question 7: Why doesn't Eureka immediately tell Ozma where the missing piglet is?\n",
      "[00:39:48.682] [425d6589]   Question 8: How does the Wizard demonstrate his magic to the Mangaboos?\n",
      "NOTE: taking first chunk location where ~partial~ text matched.\n",
      "[00:39:48.682] [425d6589]   Question 9: What is the function of Santa Claus in The Road to Oz?\n",
      "NOTE: taking first chunk location where ~partial~ text matched.\n",
      "[00:39:48.682] [425d6589]   Question 10: How does Dorothy get to and from Oz the first two times she visits?\n",
      "[00:39:48.682] [425d6589]   Question 11: In The Emerald City of Oz, what is the first instance of the author speaking in second person? \n",
      "[00:39:48.682] [425d6589]   Question 12: When does Dorothy move permanently to Oz?\n",
      "NOTE: taking first chunk location where ~partial~ text matched.\n",
      "[00:39:48.682] [425d6589]   Question 13: Why is Ojo arrested?\n",
      "[00:39:48.682] [425d6589]   Question 14: What is the Patchwork Girl's name?\n",
      "[00:39:48.682] [425d6589]   Question 15: How does Peter leave Oz?\n",
      "NOTE: taking first chunk location where ~first sentence partial~ text matched.\n",
      "[00:39:48.682] [425d6589]   Question 16: Why does Jack call Ozma his father?\n",
      "NOTE: taking first chunk location where ~first sentence partial~ text matched.\n",
      "[00:39:48.682] [425d6589]   Question 17: Who is Zeb?\n",
      "[00:39:48.682] [425d6589]   Question 18: Who is the first character the Tin Man kills?\n",
      "[00:39:48.682] [425d6589]   Question 19: How do the Evs end up hidden?\n",
      "NOTE: taking first chunk location where ~partial~ text matched.\n",
      "[00:39:48.682] [425d6589]   Question 20: Is Toto in Book 3, Ozma of Oz?\n",
      "WARNING: no chunk was matched to the given text in 'BEST ANSWER'!\n",
      "\n",
      "================================================================================\n",
      "[00:40:35.862] [425d6589] COMPLETED 1 TESTS\n",
      "[00:40:35.862] [425d6589] Total results collected: 60\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "results_df = await run_all_tests(\n",
    "    selected_questions=selected_questions, param_combos=PARAM_COMBOS[0:1], book_urls=BOOK_URLS, skip_book_processing=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "85462aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_number</th>\n",
       "      <th>question_number</th>\n",
       "      <th>result_rank</th>\n",
       "      <th>original_query</th>\n",
       "      <th>enhanced_query</th>\n",
       "      <th>target_chunk_size</th>\n",
       "      <th>sentence_overlap</th>\n",
       "      <th>small_paragraph_length</th>\n",
       "      <th>small_paragraph_overlap</th>\n",
       "      <th>matched_chapter_title</th>\n",
       "      <th>...</th>\n",
       "      <th>correct_text_found</th>\n",
       "      <th>matched_chunk_index</th>\n",
       "      <th>expected_chunk_index</th>\n",
       "      <th>chunk_distance_from_expected</th>\n",
       "      <th>char_distance_from_expected</th>\n",
       "      <th>matched_book_title</th>\n",
       "      <th>expected_book_title</th>\n",
       "      <th>correct_book_found</th>\n",
       "      <th>avg_chunk_length</th>\n",
       "      <th>all_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>How is Glinda's appearance described when Doro...</td>\n",
       "      <td>description of Glinda's appearance when Doroth...</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>Glinda The Good Witch Grants Dorothy’s Wish</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>247</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>True</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': 894.97286821705...</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': [883, 321, 873,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>How is Glinda's appearance described when Doro...</td>\n",
       "      <td>description of Glinda's appearance when Doroth...</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>Princess Ozma Of Oz</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>297</td>\n",
       "      <td>247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Marvelous Land of Oz</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>False</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': 894.97286821705...</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': [883, 321, 873,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>How is Glinda's appearance described when Doro...</td>\n",
       "      <td>description of Glinda's appearance when Doroth...</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>How Glinda Worked A Magic Spell</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>378</td>\n",
       "      <td>247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Emerald City of Oz</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>False</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': 894.97286821705...</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': [883, 321, 873,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>How old is the Scarecrow when Dorothy finds him?</td>\n",
       "      <td>the Scarecrow's age or description when Doroth...</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>How Dorothy Saved The Scarecrow</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>8950</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>True</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': 894.97286821705...</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': [883, 321, 873,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>How old is the Scarecrow when Dorothy finds him?</td>\n",
       "      <td>the Scarecrow's age or description when Doroth...</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>The Discovery Of Oz, The Terrible</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>185</td>\n",
       "      <td>38</td>\n",
       "      <td>147</td>\n",
       "      <td>131561</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>True</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': 894.97286821705...</td>\n",
       "      <td>{'the_wonderful_wizard_of_oz': [883, 321, 873,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_number  question_number  result_rank  \\\n",
       "0            1                1            1   \n",
       "1            1                1            2   \n",
       "2            1                1            3   \n",
       "3            1                2            1   \n",
       "4            1                2            2   \n",
       "\n",
       "                                      original_query  \\\n",
       "0  How is Glinda's appearance described when Doro...   \n",
       "1  How is Glinda's appearance described when Doro...   \n",
       "2  How is Glinda's appearance described when Doro...   \n",
       "3   How old is the Scarecrow when Dorothy finds him?   \n",
       "4   How old is the Scarecrow when Dorothy finds him?   \n",
       "\n",
       "                                      enhanced_query  target_chunk_size  \\\n",
       "0  description of Glinda's appearance when Doroth...                800   \n",
       "1  description of Glinda's appearance when Doroth...                800   \n",
       "2  description of Glinda's appearance when Doroth...                800   \n",
       "3  the Scarecrow's age or description when Doroth...                800   \n",
       "4  the Scarecrow's age or description when Doroth...                800   \n",
       "\n",
       "   sentence_overlap  small_paragraph_length  small_paragraph_overlap  \\\n",
       "0                 2                     200                        2   \n",
       "1                 2                     200                        2   \n",
       "2                 2                     200                        2   \n",
       "3                 2                     200                        2   \n",
       "4                 2                     200                        2   \n",
       "\n",
       "                         matched_chapter_title  ... correct_text_found  \\\n",
       "0  Glinda The Good Witch Grants Dorothy’s Wish  ...               True   \n",
       "1                          Princess Ozma Of Oz  ...              False   \n",
       "2              How Glinda Worked A Magic Spell  ...              False   \n",
       "3              How Dorothy Saved The Scarecrow  ...              False   \n",
       "4            The Discovery Of Oz, The Terrible  ...              False   \n",
       "\n",
       "   matched_chunk_index  expected_chunk_index chunk_distance_from_expected  \\\n",
       "0                  247                   247                            0   \n",
       "1                  297                   247                          NaN   \n",
       "2                  378                   247                          NaN   \n",
       "3                   28                    38                           10   \n",
       "4                  185                    38                          147   \n",
       "\n",
       "  char_distance_from_expected          matched_book_title  \\\n",
       "0                           0  The Wonderful Wizard of Oz   \n",
       "1                         NaN    The Marvelous Land of Oz   \n",
       "2                         NaN      The Emerald City of Oz   \n",
       "3                        8950  The Wonderful Wizard of Oz   \n",
       "4                      131561  The Wonderful Wizard of Oz   \n",
       "\n",
       "          expected_book_title correct_book_found  \\\n",
       "0  The Wonderful Wizard of Oz               True   \n",
       "1  The Wonderful Wizard of Oz              False   \n",
       "2  The Wonderful Wizard of Oz              False   \n",
       "3  The Wonderful Wizard of Oz               True   \n",
       "4  The Wonderful Wizard of Oz               True   \n",
       "\n",
       "                                    avg_chunk_length  \\\n",
       "0  {'the_wonderful_wizard_of_oz': 894.97286821705...   \n",
       "1  {'the_wonderful_wizard_of_oz': 894.97286821705...   \n",
       "2  {'the_wonderful_wizard_of_oz': 894.97286821705...   \n",
       "3  {'the_wonderful_wizard_of_oz': 894.97286821705...   \n",
       "4  {'the_wonderful_wizard_of_oz': 894.97286821705...   \n",
       "\n",
       "                                          all_chunks  \n",
       "0  {'the_wonderful_wizard_of_oz': [883, 321, 873,...  \n",
       "1  {'the_wonderful_wizard_of_oz': [883, 321, 873,...  \n",
       "2  {'the_wonderful_wizard_of_oz': [883, 321, 873,...  \n",
       "3  {'the_wonderful_wizard_of_oz': [883, 321, 873,...  \n",
       "4  {'the_wonderful_wizard_of_oz': [883, 321, 873,...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b2bf82fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions with correct answer in top 3: 10/20\n",
      "Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Count how many questions had at least one result with chunk_distance <= 2\n",
    "correct_questions = results_df[\n",
    "    results_df['chunk_distance_from_expected'].apply(lambda x: isinstance(x, (int, float)) and x <= 2)\n",
    "]['question_number'].nunique()\n",
    "\n",
    "total_questions = results_df['question_number'].nunique()\n",
    "\n",
    "print(f\"Questions with correct answer in top 3: {correct_questions}/{total_questions}\")\n",
    "print(f\"Accuracy: {correct_questions/total_questions*100:.1f}%\")\n",
    "# results_df[[\"original_query\", \"matched_book_title\", \"matched_chapter_title\", \"expected_chapter_title\", \"matched_chapter_number\", \"expected_chapter_number\", \"correct_chapter_found\", \"chapter_distance_from_expected\", \"matched_chunk_in_chapter_index\", \"exptected_chunk_in_chapter_index\", \"chunk_in_chap_distance_from_expected\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd6eb8",
   "metadata": {},
   "source": [
    "There are two main parameters we aim to adjust for:\n",
    "1. Semantic similarity of chunks\n",
    "2. Chunk size consistency\n",
    "\n",
    "Aiming for semantic similarity has benefits of each chunk maintaining contextual information, but then the sizes of each chunk can vary drastically. Aiming for chunk size consistency is to reduce bias towards longer chunks. Very long chunks dominate rankings simply because they contain more tokens that may match your query. Additionally, too small of a chunk and you will find poor semantic representation. Too large of a chunk and you are increasing the noise and can dilute the signal. Lastly, more constitent chunks means better embedding and search performance since the resources required are more predictable chunk by chunk.\n",
    "\n",
    "Therefore we can take advantage of book structure and but also tune for more consistent chunks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "65dfd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = ['all_chunks', 'avg_chunk_length']\n",
    "cols_to_keep = [col for col in results_df.columns if col not in cols_to_exclude]\n",
    "results_df[cols_to_keep].to_csv(\"results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
